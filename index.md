---
title: Latent Timbre Interpolation with Differentiable Digital Signal Processing
description: Hugo Flores Garcia and Patrick O'Reilly
---

## Contents

* [Introduction](#intro)  
* [Timbre Transfer Baselines](#baselines)  
* [Time-Varying Latent Timbre Interpolation](#time-varying)  
  * [Synthesizer Control Interpolation](#control)  
  * [Spectral Feature Interpolation](#spectral)  
* [Stationary Latent Timbre Interpolation](#stationary)  
  * [Single-Frame Spectral Feature Interpolation](#frame)  
  * [Textural Feature Interpretation](#texture)  
* [Future Directions](#future)

## Introduction <a name="intro"> </a>

[Differentiable Digital Signal Processing](https://magenta.tensorflow.org/ddsp) (DDSP) is a library of audio signal-processing modules implemented for use in deep learning applications. Released in 2020 by a team of researchers at [Google Magenta](https://magenta.tensorflow.org/), the project allows for computationally-efficient modeling of monophonic musical audio by constraining generation in terms of classical synthesis techniques. This serves as a useful inductive bias, allowing for high-quality emulations of musical instruments with relatively small neural network architectures.

By way of analogy, imagine you are building a neural network to generate paintings. You might consider training the network to generate pixel-by-pixel in the image domain via autoregression, transposed convolutions, or fully-connected layers. Modeling paintings at this level of granularity will likely be computationally expensive, and the network will have to learn implicitly to mimic the "textural" attributes that arise during the physical process of painting.



<div style="text-align:center">
  <img src="https://i.imgur.com/ph1wgEd.png" width=200px>
</div>





On the other hand, imagine you could constrain your network to operate on its digital canvas via brushstrokes. If you could efficiently parameterize this "generating action," you might be able to more accurately mimic some of the textural or physical aspects of painting while reducing your model's complexity.



<div style="text-align:center">
  <img src="https://i.imgur.com/fM2Tylq.png" width=200px>
</div>





In the same vein, DDSP allows for the generation of waveform audio using a high-level, physically-inspired model of sound called [Spectral Modeling Synthesis](https://ccrma.stanford.edu/~jos/sasp/Spectral_Modeling_Synthesis.html). Under this model, audio is represented as a time-varying combination of harmonics (sinusoids) and filtered noise. Convolutional reverberation is also applied to mimic the effects of different acoustic environments. DDSP implements these three components – harmonic audio, filtered noise, and reverb – as differentiable modules in the TensorFlow library. Both the harmonic and noise modules map low-dimensional "control" signals to audio, while the reverb module maps audio to audio.

DDSP modules were first demonstrated as part of an auto-encoder architecture trained to reconstruct monophonic musical instrument sounds. The architecture, pictured below, processes audio as follows: 
* an input signal is broken into short segments called frames
* loudness, pitch, and (optionally) spectral features are extracted from each frame
* these features are passed to a recurrent encoder to produce a compressed representation
* the original pitch and loudness features are concatenated with the compressed representation and passed to a recurrent decoder which generates low-dimensional control signals to parameterize the harmonic and noise modules
* the output of the harmonic and noise modules is summed and passed through the reverb module, producing waveform audio of the same length as the input. 

The model is trained end-to-end with a multi-scale spectrogram reconstruction loss. Because the autoencoder learns to map pitch and loudness curves to realistic performances of a target instrument, any monophonic audio can be re-rendered as if performed by the target instrument by passing extracted pitch and loudness curves to the model. We refer to this as __timbre transfer__, where timbre refers to the set of sonic attributes which characterize a given instrument outside of pitch and loudness. You can try this for yourself with Magenta's [Tone Transfer](https://sites.research.google/tonetransfer) app or [Colab notebooks](https://colab.research.google.com/github/magenta/ddsp/blob/master/ddsp/colab/demos/timbre_transfer.ipynb).



<div style="text-align:center">
<figure>
  <img src="https://i.imgur.com/ml4MvHX.png" width=600px>
  <figcaption style="font-size:10px;text-align:justify;"> The <b>DDSP autoencoder architecture </b>. From left to right, audio from a monophonic musical instrument is encoded as pitch ("F0") and loudness curves, and optionally as a set of spectral features ("Z"). A decoder maps the encoded input to control signals for the harmonic and noise modules. Audio generated by the modules is summed and passed through the reverb module to produce a reconstruction of the original audio.
  </figcaption>
  </figure>
</div>





A DDSP autoencoder is typically trained to reconstruct audio from a single instrument class using deterministic encoder and decoder networks, without spectral feature inputs; however, the architecture is general enough to allow for a wide range of variations.


<div style="text-align:center">
<figure>
  <img src="https://i.imgur.com/yEcGQti.png" width=500px>
  <figcaption style="font-size:10px;;text-align:justify;">The <b>DDSP reconstruction process</b>, shown left-to-right in the above autoencoder diagram, is pictured here bottom-to-top. Loudness and pitch ("Fundamental Frequency") signals are mapped by a deterministic encoder and decoder to control signals ("Amplitude," "Harmonic Distribution," "Noise Magnitudes") and passed to DDSP components which generate harmonic audio ("Additive Audio") and filtered noise. The resulting signals are summed and passed through the reverberation module to produce a reconstruction of the original audio ("Full Resynthesis")</figcaption>
 </figure>
</div>




While the DDSP autoencoder was originally developed to perform timbre transfer, [experiments](https://storage.googleapis.com/ddsp/index.html#independent) in the [original paper](https://arxiv.org/pdf/2001.04643.pdf) suggest its continuous intermediate representations may allow for interpolation between the characteristics of multiple encoded instrument timbres, thereby enabling the creation of interesting "hybrid" timbres (e.g. half-flute, half-guitar). In this project, we explore  methods for performing __timbre interpolation__ using the general DDSP autoencoder architecture.


## Timbre Transfer Baselines <a name="baselines"> </a>

We begin by building a set of _decoder-only_ DDSP autoencoder models using the [ddsp_pytorch](https://github.com/acids-ircam/ddsp_pytorch) library, a third-party PyTorch implementation of Magenta's package. Decoder-only models do not encode spectral features from the input audio, and are trained to fit a single instrument timbre using only pitch and loudness. We use a pretrained [CREPE](https://github.com/marl/crepe) pitch extractor to estimate pitch curves and use fixed DSP techniques to extract loudness curves. 


<div style="text-align:center">
<figure>
  <img src=https://i.imgur.com/w3HqLRd.png width=600px>
  <figcaption style="font-size:10px;;text-align:justify;">A <b>DDSP decoder-only model</b>. Pitch ("F0") is encoded using the CREPE pitch-tracking algorithm. Because the model is trained on a single instrument with only pitch and loudness conditioning, the decoder learns to produce a single timbre</figcaption>
 </figure>
</div>




We train models on single-instrument subsets of Google's [NSynth dataset](https://magenta.tensorflow.org/datasets/nsynth), each consisting of a small number of 4-second single-note excerpts from commercial sample libraries. For ease of comparison, we provide  timbre transfer examples using the following source audio:


__INSERT SINGNG MP3__



__TODO: FILL OUT THIS TABLE WITH THE ACTUAL DECODERS USED!!!!!__

| Instrument | Training Examples | CREPE Pitch (avg, s.d.) | Example Output |
|---|---|---|---|
| Reed | 418 | 212 ± 188 |  |
| Strings | 394 | 98 ± 47 |  | 




Additionally, we train decoder-only models on expressive solo violin and clarinet datasets, each consisting of longer and more varied excerpts than NSynth.




| Instrument | Training Examples | CREPE Pitch (avg, s.d.) | Example Output |
|---|---|---|---|
| clarinet |251| 411 ± 342  |   | 
| violin |141| 551 ± 221  |   | 




We find that to coax semi-realistic output from the baseline timbre-transfer models, a number of alterations are necessary. These include:
* **loudness**: we must carefully fine-tune the input loudness signal to match the loudness distribution the model was trained on. Even though we normalize the input loudness to match the mean and standard deviation of the training data loudness distribution, we find that models exhibit wildly different behavior when the loudness is out of distribution, especially on the expressive clarinet and violin decoders. In some cases, a compressive nonlinearity (e.g. sigmoid function) must be applied to keep inputs near a model's "sweet spot."
* **pitch**: like the authors of the original DDSP paper, we sometimes find it necessary to shift pitch curves by one or more octaves to match a model's training distribution 



## Time-Varying Latent Timbre Interpolation  <a name="time-varying"> </a>

We begin our exploration of timbre interpolation with two approaches that mix the sonic characteristics of musical instruments at time-varying representations within a DDSP autoencoder.

### Synthesizer Control Interpolation <a name="control"> </a>

One straightforward way to mix timbres is to interpolate between the control signals generated by two separately-trained single-instrument DDSP decoder-only models.



<div style="text-align:center">
<figure>
  <img src=https://i.imgur.com/tKnsYat.png width=600px>
  <figcaption style="font-size:10px;;text-align:justify;"><b>Synthesizer control interpolation</b> using two decoder-only models. The control signals emitted by both decoders are linearly interpolated and passed to the DDSP modules. </figcaption>
 </figure>
</div>



To interpolate with decoder-only models, we must train a separate decoder for each instrument we wish to interpolate between. We use the same pitch and loudness conditioning (normalized separately to match the training distributions) for both decoders. Each decoder outputs a tuple composed of the predicted harmonic amplitudes and noise filter magnitudes. We perform linear interpolation between the harmonic amplitudes of both decoders, and do the same for the noise filter magnitudes. In essence, this is equivalent to performing linear interpolation between the time-varying spectral envelopes of these two instruments. 


#### Synthesizer Control Interpolation Results

![](https://i.imgur.com/8eWDlXf.png)

**Violin**: 

<audio
    controls
    src="./audio/dcdr-vio.wav">
</audio>

**Clarinet**:
 
<audio
        controls
        src="./audio/dcdr-clar.wav">
</audio> 


**Violin -> Clarinet**:

<audio 
    controls 
    src="./audio/dcdr-vioclar.wav"> 
  </audio> 




Results for interpolation between synthesizer controls are shown above. The first two rows are reconstructions of "Somewhere over the rainbow" using only the violin and clarinet decoder-only models, respectively. The third row contains a linear interpolation between the synthesizer controls for the violin and clarinet. During the interpolation, we smoothly interpolate from the violin synthesizer controls to the clarinet synthesizer controls over the span of the audio clip. 

We find that the interpolation between the violin and clarinet timbres using synthesizer controls is rather abrupt, as the sound quickly loses the tonal qualities of the violin, overpowered by the clarinet's tonal qualities. 

This "abruptness" in interpolation is illustrated in the noise magnitude interpolation between violin and clarinet. It can be observed that the noise filter for the violin model consists of narrowband bursts of noise during string attacks. The clarinet on the other hand, comprises a much more complex noise spectrum. The interpolated noise magnitudes show that the clarinet's noise filter quickly overpowers the violin's noise filter, which can undermine the violin's timbre in the resulting interpolation. 

### Spectral Feature Interpolation <a name="spectral"> </a>

The DDSP autoencoder architecture can be modified to include a recurrent encoder which maps spectral features such as Mel-Frequency Cepstral Coefficients (MFCC) to a low-dimensional, time-varying representation of timbre. While not explicitly disentangled from the pitch and loudness representations, this additional encoder is intended to capture "residual" timbre information and allows a DDSP autoencoder to produce good reconstructions when trained on more than one instrument.


<div style="text-align:center">
<figure>
  <img src=https://i.imgur.com/sm7eWvW.png width=600px>
  <figcaption style="font-size:10px;;text-align:justify;"><b>Spectral feature interpolation</b> using one DDSP autoencoder model with a dedicated timbre encoder. Interpolation is performed between time-varying timbre representations produced by the deterministic encoder. The encoder consists of layer normalization, a gated recurrent unit (GRU), and a linear projection layer.</figcaption>
 </figure>
</div>



Using this timbre representation, the authors of the DDSP paper perform limited interpolation experiments with an autoencoder trained on two NSynth instruments. Note that because interpolation is performed between time-varying spectral features obtained from examples in the model's training distribution – rather than the input audio itself – **this approach requires matching-length input and examples of each interpolated timbre, ruling out practical real-time use**. The authors satisfy this constraint by interpolating between individual fixed-length notes from the NSynth dataset; we extend their experiments by tiling randomly-selected examples from the target timbre datasets to match the length of the input signal.


To interpolate between sequences of latent vectors ($Z$), we pick one randomly selected audio example for each instrument, and get the sequence of latent vectors for each instrument using a GRU encoder. Because these examples do not necessarily have the same length as our target audio, we take the mean of each sequence of $Z$s, and construct a sequence of arbitrary length by repeating the resulting mean $Z$ for each instrument. 


![](https://i.imgur.com/SbgMxMC.png)

**Violin**: 

<audio
    controls
    src="./audio/gru-vio.wav">
</audio> 

**Clarinet**:

<audio
    controls
    src="./audio/gru-clar.wav">
</audio> 


**Violin -> Clarinet**:

<audio 
    controls 
    src="./audio/gru-vioclar.wav"> 
  </audio> 


Results for the time-varying latent vector interpolation are shown above. Unfortunately, we find that the model disregards the latent vector when synthesizing audio, and produces perceptually similar sounds regardless of what the input $Z$ is. We believe this could be an artifact caused by simply repeating the same $Z$ vector for an entire sequence. 

## Stationary Latent Timbre Interpolation  <a name="stationary"> </a>

Interpolation between time-varying latent timbre representations poses a challenge for real-time music generation, as the input to the DDSP autoencoder must be matched step-by-step with two or more timbre signals (e.g. the spectral features of the interpolated instruments). While we can circumvent this by mixing the control signals of two or more single-instrument decoders (see "Synthesizer Control Interpolation" above), we may wish to interpolate at a higher-level latent representation in the hopes of smoothly manipulating more meaningful sonic characteristics. One way to do so is to encode timbre in a stationary (non time-varying) latent representation. Examplars of the timbres to be interpolated can then be easily encoded and mixed without any need to match the autoencoder's input signal. The following approaches tackle this stationary encoding task from different angles.

### Single-Frame Spectral Feature Interpolation <a name="frame"></a> 

This architecture is identical to the "Spectral Feature Interpolation" model shown above, except the GRU encoder is replaced by a small fully-connected network operating on a single MFCC frame (either the first or an average over all frames). Two or more timbre references are encoded, and the resulting single-step representations are interpolated, tiled to match the length of the audio input, and passed to the decoder as fixed per-time-step conditioning signals.



<div style="text-align:center">
<figure>
  <img src=https://i.imgur.com/4w9qwn2.png width=600px>
  <figcaption style="font-size:10px;;text-align:justify;"><b>Single-frame spectral feature interpolation</b> using a DDSP autoencoder model with a dedicated timbre encoder. Interpolation is performed between stationary timbre representations produced by the deterministic fully-connected encoder.</figcaption>
 </figure>
</div>



The interpolations produced by this approach are of poor quality and exhibit clear entanglement between timbre and loudness:

__INSERT AUDIO / PLOT HERE__

Interestingly, the timbre encodings themselves show reasonable separation between instrument classes; unfortunately, it appears that the decoder does not leverage this information and instead over-relies on the pitch and loudness signals.



<div style="text-align:center">
<figure>
  <img src=https://i.imgur.com/nOi0w2V.png width=600px>
  <figcaption style="font-size:10px;;text-align:justify;">Projected <b>single-frame timbre encodings</b> produced by the first and average MFCC frames. We can see that for both models, timbre encodings are reasonably well-separated. </figcaption>
 </figure>
</div>





### Textural Feature Interpolation <a name="texture"></a>

To bias our timbre encoder towards representing local, non-pitch, non-loudness sonic characteristics, we might constrain it to focus on small-scale patterns in the spectral features. While this "textural" focus will not capture meaningful large-scale timbral attributes, it may provide some degree of implicit disentanglement from the pitch/loudness representations while still capturing salient information. To this end, we adopt an approach common to image [texture-synthesis](https://papers.nips.cc/paper/2015/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf) and [style-transfer](https://arxiv.org/pdf/1508.06576.pdf) in which an input is fed through a set of convolutional filters and a "texture embedding" is computed from cross-correlations between the resulting feature maps.

Our timbre encoder consists of a set of randomly-initialized convolutional filters of various shapes, applied to the MFCC features in parallel to produce a set of time-frequency feature maps. The filter weights are fixed, and we use a large number of output channels to capture a diverse set of spectral statistics. As in image texture-synthesis and style-transfer, cross-correlations (dot products) between the flattened feature maps produced by each filter are computed and stored in [Gram matrices](https://en.wikipedia.org/wiki/Gram_matrix). We flatten and concatenate these matrices and pass them through a small fully-connected network to obtain a low-dimensional timbre representation. Interpolation is performed in the same manner as single-frame spectral feature interpolation, with the fully-connected encoder replaced by the texture encoder described here.


<div style="text-align:center">
<figure>
  <img src=https://i.imgur.com/wfjila7.png width=600px>
  <figcaption style="font-size:10px;;text-align:justify;"><b>Textural feature interpolation</b> is performed in the same manner as single-frame spectral feature interpolation, substituting the timbre encoder architecture shown here. Interpolation is performed between stationary timbre representations produced by the deterministic convolutional encoder.</figcaption>
 </figure>
</div>




This approach is somewhat ad-hoc, as there is no mechanism for enforcing smoothness or other desirable properties in the texture embedding space. As with the spectral feature interpolation, there is no mechanism for forcing the decoder to utilize the texture encodings; we simply hope that this representation is better disentangled from pitch and loudness due to the relative locality and diversity of the Gram statistics. Unfortunately, the resulting interpolations are again of poor quality despite the well-separated embedding space. 

__ADD AUDIO/PLOTS__



<div style="text-align:center">
<figure>
  <img src=https://i.imgur.com/cMf41lX.png=600px>
  <figcaption style="font-size:10px;;text-align:justify;">Projected <b>textural timbre encodings</b> produced by the random convolutional method. Again, for both models, timbre encodings are reasonably well-separated. </figcaption>
 </figure>
</div>







## Future Directions  <a name="future"> </a>

Before adopting more complex methods of encoding timbre information (e.g. Variational Autoencoders), the architecture must be modified to force the decoder to utilize timbre encodings; as we have seen, even well-separated timbre representations may not be sufficient for high-quality interpolations. One possible approach is to disentangle the timbre representation from pitch and loudness using an [adversarial loss in the encoded space](https://arxiv.org/pdf/1706.00409.pdf). Another would be to train on a much larger multi-instrument dataset, so that the decoder would not be able to easily discriminate between instrument timbres based on pitch and loudness alone.

Our initial goal was to implement timbre-interpolation in a real-time musical setting using [PureData](https://puredata.info/), and many of our architectural choices were informed by the accompanying constraints on size and speed. Besides completing a real-time timbre-interpolation implementation, future work could therefore extend the approaches detailed here to more complex architectures.
